{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Feature Extraction\n",
    "### Reads in the Pickle File of the dictionary of data, filters it, computes features, saves to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.800605Z",
     "start_time": "2019-05-15T20:13:50.430372Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import scipy\n",
    "import matplotlib\n",
    "import fabio\n",
    "import datetime\n",
    "import timeit\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import peakutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pyedflib as edf\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy import interpolate\n",
    "from datetime import datetime\n",
    "from matplotlib.collections import LineCollection\n",
    "from tqdm import tqdm\n",
    "from scipy import signal, fftpack\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from hrvanalysis import get_time_domain_features as gtdf\n",
    "from hrvanalysis import get_frequency_domain_features as gfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.809400Z",
     "start_time": "2019-05-15T20:13:51.802583Z"
    }
   },
   "outputs": [],
   "source": [
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = signal.lfilter(b, a, data)\n",
    "    return y\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.816221Z",
     "start_time": "2019-05-15T20:13:51.811349Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pickle(path,x):\n",
    "    str1 = 'Processing Data for Subject: '+x\n",
    "    print(str1)\n",
    "    print('-'*len(str1))\n",
    "    dfs = pickle.load(open(os.path.join(path,x,'Data.pickle'),'rb')) \n",
    "    ecg = pickle.load(open(os.path.join(path,x,'ECG.pickle'),'rb')) \n",
    "    ecg.loc[ecg['ECG'] == float(0),'ECG'] = np.nan\n",
    "    print('[Dictionary Loaded]')\n",
    "    return dfs, ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.830861Z",
     "start_time": "2019-05-15T20:13:51.820125Z"
    }
   },
   "outputs": [],
   "source": [
    "def reformat_ftmx(features,resolution,staging,stg):  \n",
    "    \"\"\"Combine features into 1 DataFrame\"\"\"\n",
    "#     =========================================================================================\n",
    "#     Input: features - Dictionary of feature DataFrames\n",
    "# \n",
    "#     Output: ftmx - DataFrame of all features with 'Stage' column added\n",
    "#     =========================================================================================\n",
    "#     \n",
    "#     Initialize intermediate feature dictionary\n",
    "    print('Reformatting feature data structure...'+' '*25,end='\\r')\n",
    "    features_by_stage = {}\n",
    "    for y in resolution[stg]:\n",
    "        \n",
    "#         Initialize length list\n",
    "        lens = []\n",
    "        \n",
    "#         Find shortest feature dataframe for each stage\n",
    "        for x in list(features.keys()):\n",
    "            lens.append(len(features[x][y]))\n",
    "#             print(x,features[x][y])\n",
    "        min_len = min(lens)\n",
    "#         print(min_len)\n",
    "#         Remove end rows of longer dataframes to match shorter dataframes\n",
    "        for x in list(features.keys()):\n",
    "            features[x][y].drop(features[x][y].tail(len(features[x][y])-min_len).index,inplace=True)\n",
    "        \n",
    "#         Create dataframe for all stages of each modality\n",
    "#     Initialize stage dictionary \n",
    "    stgs = {}\n",
    "    for x in list(features.keys()):\n",
    "        \n",
    "#         Initialize DataFrame for modality [x]\n",
    "        cols = features[x][0].columns.values.tolist()\n",
    "        features_by_stage[x] = pd.DataFrame(columns=cols)\n",
    "        \n",
    "#         Create DataFrame for each stage\n",
    "        for y in resolution[stg]:\n",
    "            features_by_stage[x] = features_by_stage[x].append(features[x][y])\n",
    "            stgs[y] = pd.DataFrame(y,index=list(range(len(features[x][y]))),columns=['Stage'])\n",
    "#             print(features[x][y])\n",
    "            features_by_stage[x].reset_index(inplace=True,drop=True)\n",
    "    \n",
    "#     Concat DataFrames for all stages and add staging to ftmx\n",
    "    stgs = pd.concat(stgs.values(),ignore_index=False)\n",
    "    stgs.reset_index(inplace=True,drop=True)\n",
    "    ftmx = pd.concat(features_by_stage.values(),ignore_index=False,axis=1)\n",
    "    ftmx.loc[:,'Stage'] = stgs.values\n",
    "    print('[Features Restructured]                                           ')\n",
    "    return ftmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.840621Z",
     "start_time": "2019-05-15T20:13:51.833791Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_all(ftmx,sub,res,modifier=''):\n",
    "    '''Saves feature matrix to pickle file and CSV'''\n",
    "    \n",
    "    path_pickle = os.path.join(r'F:\\Work\\Inpatient Sensors\\Data\\Processed',str(modifier),sub)\n",
    "    \n",
    "    if not os.path.exists(path_pickle):\n",
    "        os.makedirs(path_pickle)\n",
    "        \n",
    "    len_init = len(ftmx) \n",
    "    ftmx.dropna(axis=0,inplace=True)\n",
    "    len_end = len(ftmx)\n",
    "#     print('Removed ['+str(len_init-len_end)+'/'+str(len_init)+'] rows due to ECG'+' '*25)\n",
    "    write_dir = os.path.join(path_pickle,'Features'+str(res)+'.pickle')\n",
    "    pickle.dump(ftmx,open(write_dir,'wb'),protocol=-1)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.879662Z",
     "start_time": "2019-05-15T20:13:51.851386Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_acc(dfs,xx): \n",
    "    '''Applies HighPass filter with cutoff frequency of 1Hz'''\n",
    "    \n",
    "    for y in ['X','Y','Z']:\n",
    "        dfs[xx].loc[:,'Filtered_'+str(y)] = butter_highpass_filter(dfs[xx][y],1,62.5,order=1)\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "def features_acc(dfs,features,acc_features,xx,resolution,staging,stg,fs=62.50,window=120):\n",
    "# =========================================================================================\n",
    "    '''Computes the features from [acc_features] and puts them in a dataframe in [features]'''\n",
    "# =========================================================================================\n",
    "#     Initialize list of sleep stages for separation of data\n",
    "\n",
    "    stages = resolution[stg]\n",
    "    \n",
    "    \n",
    "#     Initialize features dictionary for modality [xx]\n",
    "    features[xx] = {}\n",
    "    \n",
    "#     Calculate number of datapoints in one clip based on the [window] size\n",
    "    clip_size = int(fs*window)\n",
    "    \n",
    "    if xx == 'ACCR':\n",
    "        sd = 'R'\n",
    "    elif xx == 'ACCL':\n",
    "        sd = 'L'\n",
    "    \n",
    "#     Denote what data to use for feature extraction\n",
    "    data_x = 'Filtered_X'\n",
    "    data_y = 'Filtered_Y'\n",
    "    data_z = 'Filtered_Z'\n",
    "    \n",
    "    for x in stages: # Loop through stages [0,2,5]\n",
    "        \n",
    "#         Calculate total datapoints per stage, divide by clip size\n",
    "        num_datapoints = len(dfs[xx].loc[dfs[xx]['Stage'].isin(staging[stg][x])])\n",
    "        num_clips = math.floor(num_datapoints/clip_size)\n",
    "        trimmed = dfs[xx].loc[dfs[xx]['Stage'].isin(staging[stg][x])]\n",
    "        \n",
    "#         Initialize features matrix for modality [xx] and stage [x] with columns of [acc_features]\n",
    "        features[xx][x] = pd.DataFrame() ##columns=acc_features\n",
    "        \n",
    "#         Compute features for each clip for [num_clips]\n",
    "        for y in range(num_clips):\n",
    "        \n",
    "#             Select clip of [clip_size] datapoints\n",
    "            clip = trimmed.iloc[(0+clip_size*y):(clip_size+clip_size*y),:]\n",
    "            \n",
    "#             Compute Fourier Transform of clip\n",
    "            f_data_x = scipy.fftpack.fft(clip.loc[:,data_x].values.tolist())\n",
    "            f_data_y = scipy.fftpack.fft(clip.loc[:,data_y].values.tolist())\n",
    "            f_data_z = scipy.fftpack.fft(clip.loc[:,data_z].values.tolist())\n",
    "    \n",
    "# ************************************************************************************************\n",
    "#             Time Domain\n",
    "# ************************************************************************************************\n",
    "\n",
    "#             Mean\n",
    "#             features[xx][x].loc[y,'Mean'] = clip.loc[:,'Normalized_Magnitude'].mean()\n",
    "            features[xx][x].loc[y,'Mean_X_'+sd] = clip.loc[:,data_x].mean()\n",
    "            features[xx][x].loc[y,'Mean_Y_'+sd] = clip.loc[:,data_y].mean()\n",
    "            features[xx][x].loc[y,'Mean_Z_'+sd] = clip.loc[:,data_z].mean()\n",
    "            \n",
    "#             Range\n",
    "            features[xx][x].loc[y,'Range_X_'+sd] = max(clip.loc[:,data_x]) - min(clip.loc[:,data_x]) \n",
    "            features[xx][x].loc[y,'Range_Y_'+sd] = max(clip.loc[:,data_y]) - min(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Range_Z_'+sd] = max(clip.loc[:,data_z]) - min(clip.loc[:,data_z])\n",
    "        \n",
    "#             Interquartile Range\n",
    "            features[xx][x].loc[y,'IQR_X_'+sd] = scipy.stats.iqr(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'IQR_Y_'+sd] = scipy.stats.iqr(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'IQR_Z_'+sd] = scipy.stats.iqr(clip.loc[:,data_z])\n",
    "            \n",
    "#             Standard Deviation\n",
    "            features[xx][x].loc[y,'STD_X_'+sd] = np.std(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'STD_Y_'+sd] = np.std(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'STD_Z_'+sd] = np.std(clip.loc[:,data_z])\n",
    "            \n",
    "#             Skew\n",
    "            features[xx][x].loc[y,'Skew_X_'+sd] = scipy.stats.skew(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'Skew_Y_'+sd] = scipy.stats.skew(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Skew_Z_'+sd] = scipy.stats.skew(clip.loc[:,data_z])\n",
    "            \n",
    "#             Kurtosis\n",
    "            features[xx][x].loc[y,'Kurtosis_X_'+sd] = scipy.stats.kurtosis(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'Kurtosis_Y_'+sd] = scipy.stats.kurtosis(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Kurtosis_Z_'+sd] = scipy.stats.kurtosis(clip.loc[:,data_z])\n",
    "            \n",
    "#             RMS\n",
    "            features[xx][x].loc[y,'RMS_X_'+sd] = np.sqrt(np.mean(clip.loc[:,data_x]**2))\n",
    "            features[xx][x].loc[y,'RMS_Y_'+sd] = np.sqrt(np.mean(clip.loc[:,data_y]**2))\n",
    "            features[xx][x].loc[y,'RMS_Z_'+sd] = np.sqrt(np.mean(clip.loc[:,data_z]**2))\n",
    "    \n",
    "#             Variance\n",
    "            features[xx][x].loc[y,'Variance_X_'+sd] = np.var(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'Variance_Y_'+sd] = np.var(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Variance_Z_'+sd] = np.var(clip.loc[:,data_z])\n",
    "            \n",
    "#             Min\n",
    "            features[xx][x].loc[y,'Min_X_'+sd] = min(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'Min_Y_'+sd] = min(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Min_Z_'+sd] = min(clip.loc[:,data_z])\n",
    "            \n",
    "#             Max\n",
    "            features[xx][x].loc[y,'Max_X_'+sd] = max(clip.loc[:,data_x])\n",
    "            features[xx][x].loc[y,'Max_Y_'+sd] = max(clip.loc[:,data_y])\n",
    "            features[xx][x].loc[y,'Max_Z_'+sd] = max(clip.loc[:,data_z])\n",
    "            \n",
    "#             Pearson Coefficient + P-value from Pearson Coefficient\n",
    "            features[xx][x].loc[y,'Pearson_Coeff_X-Y_'+sd], features[xx][x].loc[y,'Pearson_Pval_X-Y_'+sd] = \\\n",
    "                scipy.stats.pearsonr(clip.loc[:,data_x],clip.loc[:,data_y])   \n",
    "            features[xx][x].loc[y,'Pearson_Coeff_Y-Z_'+sd], features[xx][x].loc[y,'Pearson_Pval_Y-Z_'+sd] = \\\n",
    "                scipy.stats.pearsonr(clip.loc[:,data_y],clip.loc[:,data_z])\n",
    "            features[xx][x].loc[y,'Pearson_Coeff_Z-X_'+sd], features[xx][x].loc[y,'Pearson_Pval_Z-X_'+sd] = \\\n",
    "                scipy.stats.pearsonr(clip.loc[:,data_z],clip.loc[:,data_x])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.889423Z",
     "start_time": "2019-05-15T20:13:51.881614Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_acc(dfs,features,resolution,staging,stg,x):\n",
    "#     =========================================================================================\n",
    "    '''Process ACC data for saving to features matrix'''\n",
    "#     =========================================================================================\n",
    "\n",
    "#     list of all features to compute for accelerometer data\n",
    "    if x == 'HCS002':\n",
    "        acc_sensors = ['ACCR']\n",
    "    else:\n",
    "        acc_sensors = ['ACCL','ACCR']\n",
    "    acc_features = ['Mean_X','Mean_Y','Mean_Z',  #'Mean',\n",
    "                    'Min_X','Min_Y','Min_Z','Max_X','Max_Y','Max_Z',\n",
    "                    'Range_X','Range_Y','Range_Z','IQR_X','IQR_Y','IQR_Z',\n",
    "                    'STD_X','STD_Y','STD_Z','Kurtosis_X','Kurtosis_Y','Kurtosis_Z',\n",
    "                    'RMS_X','RMS_Y','RMS_Z','Variance_X','Variance_Y','Variance_Z',\n",
    "                    'Pearson_Coeff_X-Y','Pearson_Coeff_Y-Z','Pearson_Coeff_Z-X',\n",
    "                    'Pearson_Pval_X-Y','Pearson_Pval_Y-Z','Pearson_Pval_Z-X']\n",
    "    \n",
    "#     List of accelerometer data features can be computed from\n",
    "    acc_processed_data = ['X','Y','Z',\n",
    "                          'Filtered_X','Filtered_Y','Filtered_Z',\n",
    "                          'Normalized_Filtered_X','Normalized_Filtered_Y',\n",
    "                          'Normalized_Filtered_Z',\n",
    "                          'Magnitude','Normalized_Magnitude']\n",
    "    \n",
    "#     adds columns of filtered data to dataframe\n",
    "    for x in acc_sensors:\n",
    "#         Preprocess accelerometry data\n",
    "        \n",
    "        print('Preprocessing '+x+' '*35,end='\\r')\n",
    "        dfs = filter_acc(dfs,x)\n",
    "\n",
    "#         Extract features and place into features['ACCL','ACCR']\n",
    "        print(' - Extracting '+'['+str(len(acc_features))+']'+' Accelerometer features',end='\\r')\n",
    "        features = features_acc(dfs,features,acc_features,x,resolution,staging,stg)\n",
    "    \n",
    "    print('[Accelerometer features added to Dictionary]'+' '*25)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.903085Z",
     "start_time": "2019-05-15T20:13:51.892351Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_ecg(ecg,features,resolution,staging,y,fs=1000,window=120):\n",
    "#     ecg_features = ['HR','RMSSD','PNN50'] ## ,'HF','LF'\n",
    "    ecg_features = ['mean_nni','sdnn','sdsd','rmssd','median_nni','nni_50','pnni_50','nni_20',\n",
    "                    'pnni_20','range_nni','cvsd','cvnni','mean_hr','max_hr','min_hr','std_hr',\n",
    "                    'total_power','vlf','lf','hf','lf_hf_ratio','lfnu','hfnu'] \n",
    "    stages = resolution[y]\n",
    "#     print(stages)\n",
    "#     print(stages+' '*25,end='\\r')\n",
    "    features['ECG'] = {}\n",
    "    \n",
    "    clip_size = int(fs*window)\n",
    "    xx = 'ECG'\n",
    "    for aa in stages:\n",
    "        num_datapoints = len(ecg.loc[ecg['Stage'].isin(staging[y][aa])])\n",
    "        num_clips = math.floor(num_datapoints/clip_size)\n",
    "        trimmed = ecg.loc[ecg['Stage'].isin(staging[y][aa])]\n",
    "        \n",
    "#         Initialize features matrix for modality [xx] and stage [x] with columns of [acc_features]\n",
    "        features[xx][aa] = pd.DataFrame(columns=ecg_features)\n",
    "        \n",
    "        for z in range(num_clips):\n",
    "            clip = trimmed.iloc[(0+clip_size*z):(clip_size+clip_size*z),:]\n",
    "            clip.reset_index(inplace=True,drop=True)\n",
    "            \n",
    "            a = clip.loc[clip['Peak']==1,:].index.values\n",
    "            rr_int = [t - s for s, t in zip(a, a[1:]) if (t-s) < 1800]\n",
    "            if len(rr_int)<5:\n",
    "                for ft in ecg_features:\n",
    "                    features[xx][aa].loc[z,ft] = np.nan\n",
    "            else:\n",
    "                time1 = gtdf(rr_int)\n",
    "                freq1 = gfdf(rr_int)\n",
    "                allfts = dict(list(time1.items()) + list(freq1.items()))\n",
    "                for ft in ecg_features:\n",
    "                    features[xx][aa].loc[z,ft] = allfts[ft]\n",
    "    del ecg\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T20:13:51.923583Z",
     "start_time": "2019-05-15T20:13:51.905038Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_temp(dfs,features,resolution,staging,stg):\n",
    "    print('Processing TEMP'+' '*25,end='\\r')\n",
    "#     list of all features to compute for accelerometer data\n",
    "#     ### Removed slope   ,'Slope'\n",
    "    temp_features = ['Mean','Min','Max','Range']\n",
    "    data_cols = ['Distal','Proximal','Gradient','LDH','RDH','MC']\n",
    "    \n",
    "    print(' - Extracting ['+str(int(len(temp_features)*len(data_cols)))+'] Temperature features'+' '*25,end='\\r')\n",
    "    \n",
    "#     Get features for temp\n",
    "    features = features_temp(dfs,features,data_cols,temp_features,resolution,staging,stg)\n",
    "    \n",
    "    print('[Temperature features added to Dictionary]'+' '*25)\n",
    "    return features\n",
    "\n",
    "def features_temp(dfs,features,data_cols,temp_features,resolution,staging,stg,fs=1/60,window=120):\n",
    "    \n",
    "#     Initialize list of sleep stages for separation of data\n",
    "    stages = resolution[stg]\n",
    "#     stages = [0,2,5] # {0:'Awake',2:'Asleep',5:'REM'}\n",
    "    \n",
    "#     Initialize features dictionary for modality [xx]\n",
    "    features['TEMP'] = {}\n",
    "    xx = 'TEMP'\n",
    "#     Calculate number of datapoints in one clip based on the [window] size\n",
    "    clip_size = int(fs*window)   \n",
    "    \n",
    "    cols = []\n",
    "    for y in data_cols:\n",
    "        cols.append('Mean_'+y)\n",
    "        cols.append('Max_'+y) \n",
    "        cols.append('Min_'+y) \n",
    "        cols.append('Range_'+y)\n",
    "#         cols.append('Slope_'+y)\n",
    "    \n",
    "    for x in stages: # Loop through stages [0,1,2,3,5]\n",
    "            \n",
    "#         Calculate total datapoints per stage, divide by clip size\n",
    "        num_datapoints = len(dfs[xx].loc[dfs[xx]['Stage'].isin(staging[stg][x])])\n",
    "        num_clips = math.floor(num_datapoints/clip_size)\n",
    "        trimmed = dfs[xx].loc[dfs[xx]['Stage'].isin(staging[stg][x])]\n",
    "         \n",
    "                \n",
    "#         Initialize features matrix for modality [xx] and stage [x] with columns of [acc_features]\n",
    "        features[xx][x] = pd.DataFrame(columns=cols)\n",
    "        for y in data_cols:\n",
    "            for z in range(num_clips):\n",
    "                clip = trimmed.iloc[(0+clip_size*z):(clip_size+clip_size*z),:]\n",
    "                clip.reset_index(inplace=True,drop=True)\n",
    "                \n",
    "                features[xx][x].loc[z,'Mean_'+y] = clip.loc[:,y].mean()\n",
    "                features[xx][x].loc[z,'Max_'+y] = max(clip.loc[:,y])\n",
    "                features[xx][x].loc[z,'Min_'+y] = min(clip.loc[:,y])\n",
    "                features[xx][x].loc[z,'Range_'+y] = max(clip.loc[:,y]) - min(clip.loc[:,y])\n",
    "#                 features[xx][x].loc[z,'Slope_'+y] = ((clip.loc[1,y] - clip.loc[0,y])/60)\n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:19:05.425680Z",
     "start_time": "2019-05-15T21:05:01.599261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for Subject: HCS012\n",
      "-----------------------------------\n",
      "[Dictionary Loaded]\n",
      "[Accelerometer features added to Dictionary]                         \n",
      "[Temperature features added to Dictionary]                         \n",
      "[Features Restructured]                                           \n",
      "Removed [0/196] rows due to ECG                         \n",
      "[Accelerometer features added to Dictionary]                         \n",
      "[Temperature features added to Dictionary]                         \n",
      "[Features Restructured]                                           \n",
      "Removed [0/195] rows due to ECG                         \n",
      "[Accelerometer features added to Dictionary]                         \n",
      "[Temperature features added to Dictionary]                         \n",
      "[Features Restructured]                                           \n",
      "Removed [0/195] rows due to ECG                         \n",
      "[Accelerometer features added to Dictionary]                         \n",
      "[Temperature features added to Dictionary]                         \n",
      "[Features Restructured]                                           \n",
      "Removed [0/196] rows due to ECG                         \n"
     ]
    }
   ],
   "source": [
    "working_dir = os.path.dirname(__file__)\n",
    "rel_path = '\\HC-Sleep\\PickleFiles'\n",
    "path = os.path.join(working_dir,rel_path)\n",
    "\n",
    "dont_use = []\n",
    "subjects = [x for x in sorted(os.listdir(path)) if x not in dont_use]\n",
    "\n",
    "resolution = {'Min':[0,1],'Med':[0,2,5],'Max':[0,1,3,5],'Sws':[0,3]}\n",
    "staging = {'Min':{0:[0,0],1:[1,2,3,5]},\n",
    "           'Med':{0:[0,0],2:[1,2,3],5:[5,5]},\n",
    "           'Max':{0:[0,0],1:[1,2],3:[3,3],5:[5,5]},\n",
    "           'Sws':{0:[0,1,2,5],3:[3,3]}}\n",
    "\n",
    "# Loop through all subjects\n",
    "for x in subjects:\n",
    "    clear_output()\n",
    "#     Load data pickle file\n",
    "    dfs, ecg = load_pickle(path,x)\n",
    "\n",
    "#     Loop through different staging resolutions\n",
    "    for y in list(resolution.keys()):\n",
    "        \n",
    "        features = {}\n",
    "        features = process_acc(dfs,features,resolution,staging,y,x)\n",
    "        features = process_temp(dfs,features,resolution,staging,y)\n",
    "        features = process_ecg(ecg,features,resolution,staging,y)\n",
    "        \n",
    "        ftmx = reformat_ftmx(features,resolution,staging,y)\n",
    "        \n",
    "        save_all(ftmx,x,y,modifier='HC-Sleep')\n",
    "\n",
    "    del dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 85,
   "position": {
    "height": "40px",
    "left": "418px",
    "right": "20px",
    "top": "358px",
    "width": "445px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
